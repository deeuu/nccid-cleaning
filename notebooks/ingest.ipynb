{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import nccid_cleaning.etl as etl\n",
    "from nccid_cleaning import clean_data_df, patient_df_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can be used to generate CSV files containing patient clinical data, and image metadata for each patient and image file within the NCCID data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use these tools you need to provide a `BASE_PATH` that points to the location of the data that has been pulled from the NCCID S3 bucket, where your local directory structure should match the original S3 structure. If you have split the data into training/test/validation sets, each subdirectory should have the same structure as the original S3 bucket and the below pipeline should be run separately for each of the dataset splits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can replace the path value for `training_data` under the `Paths` section of the `config.ini` file to run the code below. This should point to the location of your NCCID data. Alternatively, comment out the first two lines and specify directly as string e.g., `Path(\"/project/data/training/\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser() #comment out if specifying directly\n",
    "config.read(\"../config.ini\") #comment out if specifying directly\n",
    "# Change value of \"training_data\" in config.ini file \n",
    "BASE_PATH = Path(config[\"Paths\"][\"training_data\"])\n",
    "print(f\"Location of NCCID data to be processed: {BASE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaging Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the imaging metadata, a separate CSV is generated for each imaging modality: X-ray, CT, MRI. Three steps are performed:\n",
    "<l>\n",
    "    <li> `select_image_files` - traverses the directory tree finding all files of the imaging modality. For X-ray is it recommended to set `select_all = True` to process all available X-ray files. Whereas, for 3D modalities, CT, and MRI, `select_first = True` is recommened to select only the first file of each imaging volume, to speed up run time and reduce redundancy of information. </li>\n",
    "    <li> `ingest_dicom_jsons` - reads the DICOM header information for each file. </li>\n",
    "    <li> `pydicom_to_df` - converts the DICOM metadata into a pandas DataFrame where the rows are images and columns are the DICOM attributes. \n",
    "</l> <br>\n",
    "\n",
    "The resulting DataFrames are saved as CSV files in `data/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subdirectories\n",
    "XRAY_SUBDIR = \"xray-metadata\"\n",
    "CT_SUBDIR = \"ct-metadata\"\n",
    "MRI_SUBDIR = \"mri-metadata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. finding image file lists within the subdirs\n",
    "xray_files = etl.select_image_files(BASE_PATH / XRAY_SUBDIR, select_all=True)\n",
    "ct_files = etl.select_image_files(BASE_PATH / CT_SUBDIR, select_first=True)\n",
    "mri_files = etl.select_image_files(BASE_PATH / MRI_SUBDIR, select_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. process image metadata\n",
    "xray_datasets = etl.ingest_dicom_jsons(xray_files)\n",
    "ct_datasets = etl.ingest_dicom_jsons(ct_files)\n",
    "mri_datasets = etl.ingest_dicom_jsons(mri_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. converting to DataFrame\n",
    "xrays = etl.pydicom_to_df(xray_datasets)\n",
    "cts = etl.pydicom_to_df(ct_datasets)\n",
    "mris = etl.pydicom_to_df(mri_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check structure of DFs\n",
    "xrays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as csv\n",
    "xrays.to_csv(\"data/xrays.csv\")\n",
    "cts.to_csv(\"data/cts.csv\")\n",
    "mris.to_csv(\"data/mris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient Clinical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For patient clinical data, the most recent <b>data</b> file (for COVID-positive) or <b>status</b> file (for COVID-negative) is parsed for each patient in the directory tree. The resulting DataFrame is generated using `patient_jsons_to_df`, where rows are patients and columns are data fields. <br>\n",
    "\n",
    "Three fields that are not in the original jsons files are included in the DataFrame: \n",
    "<l>\n",
    "    <li> `filename_earliest_date` - earlist data/status file present for the patient. </li>\n",
    "    <li> `filename_latest_date` - latest data/status file present for the patient. This is the file from which the rest of the patient's data has been pulled. </li>\n",
    "    <li> `filename_covid_status` - indicates it the patient is in the COVID-postive or COVID-negative cohort, based on whether they have every been submitted with a <b>data</b> file (which are only present for positive patients. </li>\n",
    " </l>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENT_SUBDIR = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process patient clinical data\n",
    "patient_files = list(os.walk(BASE_PATH / PATIENT_SUBDIR))\n",
    "patients = etl.patient_jsons_to_df(patient_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and enrich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaning pipeline can be run on the resulting patients DataFrame to improve quality. In addition, missing values in the patient DataFrame for Sex and Age, can be filled using the DICOM image headers. This step generates two new columns `sex_update` and `age_update`, from the cleaned columns `sex`, `age`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning\n",
    "patients = clean_data_df(patients, patient_df_pipeline)\n",
    "\n",
    "# enriching\n",
    "images = [xrays, cts, mris] # list all image DFs\n",
    "patients = etl.patient_data_dicom_update(patients, images)\n",
    "patients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sex Unknowns before merging with dicom: {(patients['sex']=='Unknown').sum()}\")\n",
    "print(f\"Sex Unknowns after merging with dicom: {(patients['sex_update']=='Unknown').sum()}\")\n",
    "print(\"------\")\n",
    "print(f\"Age NaNs before merging with dicom: {patients['age'].isnull().sum()}\")\n",
    "print(f\"Age New after merging with dicom: {patients['age_update'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "patients.to_csv(\"data/patients.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
